## Notebooks and helpers for the "Adversaries examples & human-ML alignment" tutorial.

[Exercise notebook](https://colab.research.google.com/github/MadryLab/AdvEx_Tutorial/blob/master/alignment_exercise.ipynb)

[Solution notebook](https://colab.research.google.com/github/MadryLab/AdvEx_Tutorial/blob/master/alignment_solution.ipynb)

### Based on the following works
[IST+19]	Ilyas A., Santurkar S., Tsipras D., Engstrom L., Tran B., Madry A. (2019). Adversarial Examples Are Not Bugs, They Are Features. arXiv, arXiv:1905.02175

[EIS+19]	Engstrom L., Ilyas A., Santurkar S., Tsipras D., Tran B., Madry A. (2019). Learning Perceptually-Aligned Representations via Adversarial Robustness. arXiv, arXiv:1906.00945

[STE+19]	Santurkar S., Tsipras D., Tran B., Ilyas A., Engstrom L., Madry A. (2019). Image Synthesis with a Single (Robust) Classifier. arXiv, arXiv:1906.09453

[EIS+19] Robustness (Python Library) (2019); https://github.com/MadryLab/robustness.

### Citation 
If you use this material or code, please cite it as follows:

```
@misc{santurkar2020notes,
   title={Adversarial examples and human-ML alignment (MIT BCS tutorial)},
   author={Shibani Santurkar and Dimitris Tsipras},
   year={2020},
   url={https://github.com/MadryLab/BCS_Tutoria}
}
```

### Contributors
* [Shibani Santurkar](http://people.csail.mit.edu/shibani/)
* [Dimitris Tsipras](http://people.csail.mit.edu/tsipras/)
